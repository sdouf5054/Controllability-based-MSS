diff --git a/src/analysis/trackA_analysis.py b/src/analysis/trackA_analysis.py
index 0000000..1111111 100644
--- a/src/analysis/trackA_analysis.py
+++ b/src/analysis/trackA_analysis.py
@@ -10,13 +10,12 @@
 """
 
 import numpy as np
+from numpy.typing import ArrayLike
 import pandas as pd
 from pathlib import Path
-from typing import Dict, List, Optional, Tuple
-from dataclasses import dataclass, field
+from typing import Any, Dict, List, Optional, Tuple
 from scipy import stats as scipy_stats
 import json
-import warnings
 
 from ..data_types import (
     ProxyAssociation,
@@ -28,39 +27,40 @@
 
 
 def compute_correlation(
-    x: np.ndarray,
-    y: np.ndarray,
+    x: ArrayLike,
+    y: ArrayLike,
     method: str = "spearman"
 ) -> Tuple[float, float, int]:
-    """Compute correlation between two arrays.
-    
-    Args:
-        x: First array
-        y: Second array
-        method: "spearman" or "pearson"
-        
-    Returns:
-        Tuple of (correlation, p_value, n_samples)
+    """Compute correlation between two 1D arrays.
+
+    Note:
+        We coerce inputs to float numpy arrays to avoid pandas ExtensionArray issues
+        and to make NaN filtering reliable.
     """
-    # Remove NaN
-    valid = ~(np.isnan(x) | np.isnan(y))
-    x_valid = x[valid]
-    y_valid = y[valid]
-    
-    n = len(x_valid)
-    
+    x_arr = np.asarray(x, dtype=float).ravel()
+    y_arr = np.asarray(y, dtype=float).ravel()
+
+    # Remove NaN pairs
+    valid = ~(np.isnan(x_arr) | np.isnan(y_arr))
+    x_valid = x_arr[valid]
+    y_valid = y_arr[valid]
+
+    n = int(x_valid.size)
     if n < 3:
-        return float('nan'), float('nan'), n
-    
+        return float("nan"), float("nan"), n
+
     if method == "spearman":
-        rho, p = scipy_stats.spearmanr(x_valid, y_valid)
+        res = scipy_stats.spearmanr(x_valid, y_valid)
+        rho = float(res.statistic)
+        p = float(res.pvalue)
     elif method == "pearson":
-        rho, p = scipy_stats.pearsonr(x_valid, y_valid)
+        res = scipy_stats.pearsonr(x_valid, y_valid)
+        rho = float(res.statistic)
+        p = float(res.pvalue)
     else:
         raise ValueError(f"Unknown method: {method}")
-    
-    return float(rho), float(p), n
-
+
+    return rho, p, n
 
 def compute_cue_difficulty_correlation(
     cues_df: pd.DataFrame,
@@ -98,12 +98,12 @@
                     'interpretation': 'no_valid_samples'
                 }
                 continue
-            x = cue_data[cue_name].values
+            x = cue_data[cue_name].to_numpy(dtype=float, na_value=np.nan)
         else:
             cue_data = merged
-            x = merged[cue_name].values
-        
-        y = cue_data['sisdr_vocal'].values
+            x = merged[cue_name].to_numpy(dtype=float, na_value=np.nan)
+        
+        y = cue_data['sisdr_vocal'].to_numpy(dtype=float, na_value=np.nan)
         
         rho, p, n = compute_correlation(x, y, method)
         
@@ -168,8 +168,8 @@
         if proxy_name not in merged.columns:
             continue
         
-        x = merged[proxy_name].values
-        y = merged['sisdr_vocal'].values
+        x = merged[proxy_name].to_numpy(dtype=float, na_value=np.nan)
+        y = merged['sisdr_vocal'].to_numpy(dtype=float, na_value=np.nan)
         
         rho, p, n = compute_correlation(x, y, method)
         
@@ -225,8 +225,8 @@
         else:
             data = merged
         
-        est_values = data[est_col].values
-        gt_values = data[gt_col].values
+        est_values = data[est_col].to_numpy(dtype=float, na_value=np.nan)
+        gt_values = data[gt_col].to_numpy(dtype=float, na_value=np.nan)
         
         # Remove NaN
         valid = ~(np.isnan(est_values) | np.isnan(gt_values))
@@ -244,7 +244,8 @@
             continue
         
         # Compute correlation
-        corr, _ = scipy_stats.pearsonr(est_valid, gt_valid)
+        res = scipy_stats.pearsonr(est_valid, gt_valid)
+        corr = float(res.statistic)
         
         # Compute difference statistics
         diff = est_valid - gt_valid
@@ -253,7 +254,7 @@
         
         results.append(CueContamination(
             cue_name=cue_name,
-            correlation=float(corr),
+            correlation=corr,
             mean_diff=mean_diff,
             std_diff=std_diff,
             n_samples=len(est_valid)
@@ -347,6 +348,7 @@
     n_segments_valid = len(difficulty_df[difficulty_df['valid'] == True])
     
     # Difficulty statistics
+    difficulty_stats: Dict[str, Any]
     valid_difficulty = difficulty_df[difficulty_df['valid'] == True]['sisdr_vocal']
     if len(valid_difficulty) > 0:
         difficulty_stats = {
@@ -525,6 +527,34 @@
         Returns:
             TrackASummary object
         """
+        # Ensure required tables are loaded (and narrow Optional -> DataFrame for type checking)
+        if self.manifest_df is None:
+            raise FileNotFoundError(
+                f"Missing manifest_df. Expected: {self.metadata_root / 'trackA_manifest.csv'}"
+            )
+        if self.segments_df is None:
+            raise FileNotFoundError(
+                f"Missing segments_df. Expected: {self.tables_root / 'segments_trackA.parquet'}"
+            )
+        if self.difficulty_df is None:
+            raise FileNotFoundError(
+                f"Missing difficulty_df. Expected: {self.tables_root / 'difficulty_trackA.parquet'}"
+            )
+        if self.cues_df is None:
+            raise FileNotFoundError(
+                f"Missing cues_df. Expected: {self.tables_root / 'cues_trackA.parquet'}"
+            )
+        if self.proxies_df is None:
+            raise FileNotFoundError(
+                f"Missing proxies_df. Expected: {self.tables_root / 'proxies_trackA.parquet'}"
+            )
+
+        manifest_df = self.manifest_df
+        segments_df = self.segments_df
+        difficulty_df = self.difficulty_df
+        cues_df = self.cues_df
+        proxies_df = self.proxies_df
+
         # Compute correlations
         self.compute_correlations()
         
@@ -550,11 +580,11 @@
         # Generate summary
         self.summary = generate_trackA_summary(
             dataset_name=dataset_name,
-            manifest_df=self.manifest_df,
-            segments_df=self.segments_df,
-            difficulty_df=self.difficulty_df,
-            cues_df=self.cues_df,
-            proxies_df=self.proxies_df,
+            manifest_df=manifest_df,
+            segments_df=segments_df,
+            difficulty_df=difficulty_df,
+            cues_df=cues_df,
+            proxies_df=proxies_df,
             silence_result=silence_result,
             cue_correlations=self.cue_correlations,
             proxy_associations=self.proxy_associations,
